# -*- coding: utf-8 -*-
"""news_scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16foBygkZ-1EvtyElHT70ZLs7aXPZpRp_
"""
import os
import pandas as pd
import requests
from fake_useragent import UserAgent

import bs4
from datetime import date
from tqdm import tqdm


ua = UserAgent()

def fake_header():
    return {
            'User-Agent': ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
    }

tickers_list = ['AAPL.O', 'LVS', 'COTY.K','JPM', 'XOM', '005930.KS']
# data    = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies#S%26P_500_component_stocks')
# table   = data[0]
# ticker_list = list(table.Symbol.values)

publishers_list = ['finance.yahoo.com', 'cnbc.com', 'marketwatch',
                   'seekingalpha.com/article', 'reuters.com', 'barrons.com/articles']

start_date = '1/1/2021'
end_date = '5/4/2022'

def scrape_news(tickers_list, publishers_list, start_date, end_date, max_res_per_ticker=1):

    # lists to store all news search results details
    news = []
    publishers = []
    urls = []
    titles = []
    dates = []
    tickers = []

    for ticker in tqdm(tickers_list):

        # lists to store current ticker's news search results details
        t_news = []
        t_publishers = []
        t_urls = []
        t_dates = []
        t_tickers = []
        t_titles = []

        print('ticker: ', ticker)

        # clean ticker to not include '.'
        ticker = ticker.split('.', 1)[0]

        # format google query
        query = '{}+stock+news'.format(ticker)
        google_query_url = f"https://www.google.com/search?q={query}&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}&source=lnms&tbm=nws&num=25"

        # conduct request/ Google search
        r = requests.Session()
        headers = fake_header()
        res = r.get(google_query_url, headers=headers)
        print('headers: ,' , headers)

        # get all hyperlinks from search results
        soup = bs4.BeautifulSoup(res.text, "html.parser")
        search_results_links = soup.select("div a")

        for link in search_results_links:
            try:

                # get actual news page url
                news_page_url = link.get('href')

                # only open current link if its publisher is part of target publishers
                from_publisher = [ele for ele in publishers_list if (ele in news_page_url)]

                if from_publisher:
                    # ensure url is in openable form
                    if (news_page_url[0: 4] not in 'http'):
                        news_page_url = news_page_url[news_page_url.index('http'):]

                    if ('&ved' in news_page_url):
                        news_page_url = news_page_url.split('&ved', 1)[0]

                    if ('ved' in news_page_url):
                        news_page_url = news_page_url.split('ved', 1)[0]

                    if ('&sa=U' in news_page_url):
                        news_page_url = news_page_url.split('&sa=U', 1)[0]

                    # "open" news page link
                    res = requests.get(news_page_url, headers=fake_header())

                    # parse news page's html, get desired article info
                    parsed_article = bs4.BeautifulSoup(res.text, 'lxml')
                    article_title = parsed_article.find('h1').get_text()
                    paragraphs = parsed_article.find_all('p')
                    publish_date = parsed_article.find('time')['datetime']

                    # merge individual paragraphs into a single paragraph
                    article_text = " "
                    for paragraph in paragraphs:
                        article_text += (" " + paragraph.text)

                    # add current article's details to associated list
                    t_news.append(article_text)
                    t_tickers.append(ticker)
                    t_titles.append(article_title)
                    t_publishers.append(from_publisher)
                    t_urls.append(news_page_url)
                    t_dates.append(publish_date)

                # if current link's publisher not part of target publishers, skip opening it
                else:
                    continue

            except Exception as e:
                print('link: ', news_page_url)
                print('in exception: ', e)
                article_text = ""
                article_title = ""

        # finally, add all the results from the current ticker's search to related lists
        news += t_news
        tickers += t_tickers
        titles += t_titles
        publishers += t_publishers
        urls += t_urls
        dates += list(t_dates)

        print('urls', len(urls), 'news', len(news), 'publishers', len(publishers), 'tickers', len(tickers), 'titles', len(titles), 'dates', len(dates))


    # create df populated with ALL scraped articles and their details
    df = pd.DataFrame()
    df['ticker'] = tickers
    df['url'] = urls
    df['article_title'] = titles
    df['article_text'] = news
    df['publisher'] = publishers
    df['dates'] = dates

    # import to csv
    df.to_csv(f'google_news.csv')

    # delete objects
    del news, publishers, urls, dates, tickers, titles
    del t_news, t_publishers, t_urls, t_dates, t_tickers

if os.path.exists('google_news.csv'):
    df = pd.read_csv('google_news.csv', index_col=0)
    #print(df.head())
    keyword_df = df.drop(['ticker', 'publisher', 'url', 'dates'], axis=1)
    keyword_df = keyword_df.rename({'article_title': 'title', 'article_text' : 'description'}, axis=1)
    keyword_df.to_csv('ready.csv', index=False)

    kdf = pd.read_csv('ready.csv')
    print(kdf.head())
    print(kdf.loc[0])
else:
    scrape_news(tickers_list, publishers_list, start_date, end_date)

